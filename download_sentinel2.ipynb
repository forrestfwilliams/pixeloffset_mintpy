{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentinel 2 Cloudless Mosaic\n",
    "\n",
    "This notebook selects and crops a *cloudless* yearly time series of [Sentinel-2 Level-2A](https://planetarycomputer.microsoft.com/dataset/sentinel-2-l2a) images and is modified from the example notebook provided by Microsoft. This notebook performs the following steps:\n",
    "\n",
    "* Find a time series of images within a bounding box\n",
    "* Remove images that contain clouds\n",
    "* Isolate the near-infrared (NIR) band (band 8 for Sentinel-2)\n",
    "* Select a set of annual images that occur on approximately the same day of the year\n",
    "* Save the results to GeoTiffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "import rasterio\n",
    "import rasterio.features\n",
    "import rioxarray\n",
    "import stackstac\n",
    "import pystac_client\n",
    "import planetary_computer\n",
    "\n",
    "import pyproj\n",
    "from shapely.ops import transform\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "import xrspatial.multispectral as ms\n",
    "\n",
    "import dask\n",
    "from dask_gateway import GatewayCluster\n",
    "from dask import visualize\n",
    "\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from shapely.geometry import shape,box\n",
    "from skimage.morphology import dilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polygon_to_raster(gdf,template_path,value=1,crs=False):\n",
    "    if isinstance(gdf,str):\n",
    "        pol = gpd.read_file(gdf)\n",
    "    else:\n",
    "        pol = gdf\n",
    "\n",
    "    with rasterio.open(template_path) as dst:\n",
    "        profile = dst.profile\n",
    "        template_crs = dst.crs\n",
    "        template_transform = profile['transform']\n",
    "        template_shape = dst.shape\n",
    "\n",
    "    # if crs != pol.crs:\n",
    "    #   raise Exception('CRSs do not match!')\n",
    "\n",
    "    geojsons = [x['geometry'] for x in pol.geometry.__geo_interface__['features']]\n",
    "    if isinstance(value,str):\n",
    "        shapes = [tuple(x) for x in zip(geojsons,pol[value])]\n",
    "    else:\n",
    "        shapes = [(x,value) for x in geojsons]\n",
    "\n",
    "    array = rasterio.features.rasterize(shapes, out_shape=template_shape, transform=template_transform)\n",
    "    \n",
    "    result = [array, profile]\n",
    "    if crs:\n",
    "        result.append(template_crs)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def write_raster(array,profile,out_path,nodata,dtype):\n",
    "    # From rasterio docs:\n",
    "    # Register GDAL format drivers and configuration options with a\n",
    "    # context manager.\n",
    "    with rasterio.Env():\n",
    "        # And then change the band count to 1, set the\n",
    "        # dtype to uint8, and specify LZW compression.\n",
    "        profile.update(\n",
    "            dtype=dtype,\n",
    "            count=1,\n",
    "            nodata=nodata,\n",
    "            compress='lzw')\n",
    "\n",
    "        with rasterio.open(out_path, 'w', **profile) as dst:\n",
    "            dst.write(array.astype(dtype), 1)\n",
    "\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Dask cluster\n",
    "\n",
    "We're going to process a large amount of data. To cut down on the execution time, we'll use a Dask cluster to do the computation in parallel, adaptively scaling to add and remove workers as needed. See [Scale With Dask](../quickstarts/scale-with-dask.ipynb) for more on using Dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the cluster\n",
    "cluster = GatewayCluster()  # Creates the Dask Scheduler. Might take a minute.\n",
    "client = cluster.get_client()\n",
    "cluster.adapt(minimum=4, maximum=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cluster.dashboard_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discover data\n",
    "\n",
    "In this step we define our bounding box by creating a Shapely Polygon object. The Polygon object is created from a set of coordinate pairs in **Latitude and Longitude** (epsg 3857). A simple way of getting the coordinate pairs is by creating a bounding box in Google Earth, saving it to a kml, then opening it as a text file and copying the coordinates.\n",
    "\n",
    "At this point, you'll have to decide if you want to process multiple years at once, or if you want to process the years separately. This decision comes down to how much compute power you have access to. For the Dask cluster parameters specified, `cluster.adapt(minimum=4, maximum=24)`, the maximum amount of images used should be less than 200. You can alter the number of images used by changing the values of `date_range`, `max_cloud`, and `pol`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#proroa: R129, T60GUA & T60HUB\n",
    "#andrew: R129, T60GVA\n",
    "#bird: R129, T60HUB\n",
    "\n",
    "# setting options\n",
    "date_range = '2016-01-01/2022-01-01'\n",
    "local_zone = 'EPSG:32760'\n",
    "buffer_width = 2000\n",
    "max_cloud_image = 33\n",
    "max_cloud_bbox = 1\n",
    "landslide = 1\n",
    "\n",
    "# get landslide\n",
    "ls_all = gpd.read_file('landslides/landslide_sediment.shp').to_crs('EPSG:4326')\n",
    "ls = ls_all.loc[ls_all['id'] == landslide]\n",
    "\n",
    "# get bounding box (minx, miny, maxx, maxy)\n",
    "bbox = tuple(ls.total_bounds)\n",
    "\n",
    "aoi = gpd.GeoSeries([box(*bbox)],crs='EPSG:4326').to_crs(local_zone)\n",
    "aoi_buffer = aoi.buffer(buffer_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ls.explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `pystac_client` we can search the Planetary Computer's STAC endpoint for items matching our query parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stac = pystac_client.Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\")\n",
    "\n",
    "search = stac.search(\n",
    "    bbox=bbox,\n",
    "    datetime=date_range,\n",
    "    collections=[\"sentinel-2-l2a\"],\n",
    "    limit=500,  # fetch items in batches of 500\n",
    "    query={\"eo:cloud_cover\": {\"gte\":0,\"lte\": max_cloud_image}},\n",
    ")\n",
    "\n",
    "# Get items with the correct relative orbit\n",
    "items = list(search.get_items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['orbit'] = [x.id.split('_')[3] for x in items]\n",
    "df['frame'] = [x.id.split('_')[4] for x in items]\n",
    "geometries = [shape(x.geometry).wkt for x in items]\n",
    "\n",
    "gdf = gpd.GeoDataFrame(df, geometry=gpd.GeoSeries.from_wkt(geometries,crs='EPSG:4326'))\n",
    "gdf['geometry'] = gdf.geometry.to_crs(local_zone)\n",
    "gdf['overlap'] = gdf.geometry.intersection(aoi_buffer[0]).area / aoi_buffer[0].area\n",
    "gdf = gdf[gdf['overlap'] >= 0.99]\n",
    "counts = gdf.loc[:,['orbit','frame','overlap']].groupby(['orbit','frame']).count().reset_index()\n",
    "orbit, frame, count = counts.loc[counts['overlap'] == counts['overlap'].max()].values[0]\n",
    "\n",
    "image_footprint = gdf[(gdf['orbit'] == orbit) & (gdf['frame'] == frame)].iloc[:1]\n",
    "\n",
    "print(f'Selected orbit {orbit} and frame {frame} with {count} images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = image_footprint.explore()\n",
    "m = aoi_buffer.explore(m=m,style_kwds={'color':'red'})\n",
    "m = aoi.explore(m=m,style_kwds={'color':'black'})\n",
    "# m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we restrict the results to the same orbit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab ids\n",
    "ids = np.array([x.id.split('_') for x in items])\n",
    "\n",
    "# get correct orbit\n",
    "ids = ids[ids[:,3] == orbit]\n",
    "\n",
    "# get correct frame\n",
    "ids = ids[ids[:,4] == frame]\n",
    "\n",
    "# grab valid ids\n",
    "valid_ids = ['_'.join(x) for x in ids]\n",
    "\n",
    "#subset items\n",
    "items = [x for x in items if any([y == x.id for y in valid_ids])]\n",
    "\n",
    "print(f'Number of images before cloud masking is {len(items)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the location, this should return about 50-100 images for our study area over time, and cloudiness. Those items will still have *some* clouds over portions of the scenes, though. To create our cloudless mosaic, we'll load the data into an [xarray](https://xarray.pydata.org/en/stable/) DataArray using [stackstac](https://stackstac.readthedocs.io/) and then reduce the time-series of images down to a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signed_items = []\n",
    "for item in items:\n",
    "    item.clear_links()\n",
    "    signed_items.append(planetary_computer.sign(item).to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we load the data and perform some initial cleaing that includes:\n",
    "* subsetting to our exact bounding box\n",
    "* removing pixels that correspond clouds and clouds shadows\n",
    "\n",
    "To perform our cloud masking, we use Sentinel-2's Scene Classification Layer ([SCL](https://sentinels.copernicus.eu/web/sentinel/technical-guides/sentinel-2-msi/level-2a/algorithm)) and mask out values 3, 8, 9, and 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (\n",
    "    stackstac.stack(\n",
    "        signed_items,\n",
    "        assets=[\"B08\",\"SCL\"],\n",
    "        chunksize=4096*2,\n",
    "        resolution=10\n",
    "    )\n",
    "    .where(lambda x: x > 0, other=np.nan)  # sentinel-2 uses 0 as nodata\n",
    ")\n",
    "\n",
    "# Get bounding box in projection of data\n",
    "minx, miny, maxx, maxy = tuple(aoi_buffer.to_crs(data.crs).total_bounds)\n",
    "\n",
    "# Subset data and mask clouds\n",
    "data = data.sel(x=slice(minx, maxx), y=slice(maxy,miny))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloud filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = data.groupby('time').first(skipna=False)\n",
    "valid = xr.where(first.sel(band='SCL',drop=True).isin([3,8,9]),x=0,y=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_valid = valid.sum(dim=['x','y']).compute().to_numpy() / (data.shape[2] * data.shape[3])\n",
    "pct_valid = pct_valid.squeeze()\n",
    "dates = valid.time.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clouds = pd.DataFrame({'date':dates,'pct_valid':pct_valid[:]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = clouds.loc[clouds.pct_valid > (100-max_cloud_bbox)/100].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best['year'] = best.date.dt.year\n",
    "counts = best.groupby('year').count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,row in counts.iterrows():\n",
    "    print(f'Year {row[\"year\"]} contains {row[\"date\"]} images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_dates = data.sel(band=[\"B08\"],time=list(best.date)).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Closest Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_df = pd.DataFrame({'date':best.date})\n",
    "date_df['year'] = date_df.date.dt.year\n",
    "date_df['doy'] = date_df.date.dt.dayofyear\n",
    "date_df['month'] = date_df.date.dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_months = [9,10,11,12]\n",
    "summer = date_df.loc[date_df.month.isin(good_months)].copy()\n",
    "doys = [list(zip(x.date,x.doy)) for _,x in summer.groupby('year')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_distance = np.inf\n",
    "\n",
    "for x in tqdm(list(itertools.product(*doys))):\n",
    "    distance = 0\n",
    "    for d1,d2 in itertools.combinations(x,2):\n",
    "        a = d1[1]\n",
    "        b = d2[1]\n",
    "        distance += min(abs(a-b),abs(a+365-b),abs(b-a),abs(b+365-a))\n",
    "\n",
    "    if np.mean(distance) < min_distance:\n",
    "        min_distance = np.mean(distance)\n",
    "        date_list = [z[0] for z in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_df = date_df.loc[date_df.date.isin(date_list)]\n",
    "plt.scatter(date_df.doy,date_df.year)\n",
    "plt.plot(closest_df.doy,closest_df.year,color='red')\n",
    "print(min_distance,date_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dict = {}\n",
    "\n",
    "for t in closest_df.date:\n",
    "    time_name = datetime.strftime(t,'%Y%m%d')\n",
    "    name = f's2_l2_{orbit}_{frame}_{time_name}.tif'\n",
    "    img_dict[name] = best_dates.sel(time=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, [[ax1,ax2],[ax3,ax4],[ax5,ax6]] = plt.subplots(3,2,figsize=(15,25))\n",
    "axes = [ax1,ax2,ax3,ax4,ax5,ax6]\n",
    "for ax, name in zip(axes,img_dict):\n",
    "    ax.imshow(img_dict[name])\n",
    "    ax.set_title(name)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in img_dict:\n",
    "    print(f'Writing {name}...')\n",
    "    img_dict[name].rio.to_raster(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create active mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pols = ls_all.to_crs(local_zone)\n",
    "template = list(img_dict.keys())[0]\n",
    "mask, profile = polygon_to_raster(pols,template,value=1,crs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_buffered = dilation(mask,np.ones((11,11)))\n",
    "plt.imshow(mask_buffered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_raster(mask_buffered,profile,'activity_mask.tif',nodata=-32768,dtype=rasterio.int16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Close cluster\n",
    "Once we're done with our processing, let's be a good steward of our resources and close our cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data\n",
    "And you're done! The completed GeoTiff files should be in the same directory as this notebook, and can be downloaded via Jupyter's GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "2ca0804b9f904dab815db80637a4f2d9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e2f3ac516e3b4cf3a1ba1fc6aa0897ad": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "VBoxModel",
      "state": {
       "layout": "IPY_MODEL_2ca0804b9f904dab815db80637a4f2d9"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
